---
title: "DS2 HW4"
author: "Mufeng Xu"
date: "4/8/2021"
output: html_document
---
```{r setup, include=FALSE}
library(tidyverse)
library(lasso2)
library(rpart)
library(rpart.plot)
library(caret)
library(ranger)
library(gbm)
library(ISLR)
```

# Problem 1

## A. Regression tree

```{r}
data(Prostate)

# The lowest CV error 
set.seed(1)

ctrl = trainControl(method = "cv")

tree_fit_1 = train(lpsa~.,
                   data = Prostate,
                   method = "rpart",
                   tuneGrid = data.frame(cp = exp(seq(-8,-2, length = 20))),
                   trControl = trainControl(method = "cv"))

ggplot(tree_fit_1, highlight = TRUE)

tree_fit_1$bestTune
tree_fit_1$finalModel$cptable
rpart.plot(tree_fit_1$finalModel)
```


```{r}
# 1 SE rule
set.seed(1) 
tree_fit_2 = train(lpsa~.,
                   data = Prostate,
                   method = "rpart",
                   tuneGrid = data.frame(cp = exp(seq(-8, -2, length = 20))),
                   trControl = trainControl(method = "cv",
                                            number = 10,
                                            selectionFunction = "oneSE"))

ggplot(tree_fit_2, highlight = TRUE)

tree_fit_2$finalModel$cptable
rpart.plot(tree_fit_2$finalModel)
```

The regression trees from lowest CV and 1 SE rule are different.

The tree corresponding to the lowest cross-validation error has a size of 8, while the tree obtained with 1 SE rule has a size of 3

## B. Choose the decision tree

```{r}
resamp = resamples(list(lowest_cv_error = tree_fit_1, 
                         one_se = tree_fit_2
                         ))

bwplot(resamp, metric = "RMSE")

rpart.plot(tree_fit_2$finalModel)
```

The regression tree with size of 3 is chosen: 

For the terminal node lcavol < 2.5, if log(cancer volumn) is smaller than 2.5, there is 78% of probability that log(prostate specific antigen) to be 2.1. If lcovol is not smaller than 2.5, there is 22% of probability that lpsa to be 3.8.

## C. Bagging

```{r}
set.seed(1)
bagging.grid = expand.grid(mtry = 8,
                           splitrule = "variance",
                           min.node.size = 1:30)
bag_fit = train(lpsa~.,
                data = Prostate,
                method = "ranger",
                tuneGrid = bagging.grid,
                importance = "impurity",
                trControl = trainControl(method = "cv"))

ggplot(bag_fit, highlight = TRUE)

barplot(sort(ranger::importance(bag_fit$finalModel), decreasing = FALSE),
        las = 2, horiz = TRUE, cex.names = 0.7,
        col = colorRampPalette(colors = c("darkred","white","darkblue"))(19))
```

Variable Importance:
lcavol > lweight > svi > pgg45  > lcp > age > lbph > gleason

## D. Random Forests

```{r}
set.seed(1)
rf.grid = expand.grid(mtry = 1:6, splitrule = "variance", min.node.size = 1:30)

rf_fit = train(lpsa~.,
               data = Prostate,
               method = "ranger",
               tuneGrid = rf.grid,
               importance = "impurity",
               trControl = trainControl(method = "cv"))

ggplot(rf_fit, highlight = TRUE)

barplot(sort(ranger::importance(rf_fit$finalModel), decreasing = FALSE),
        las = 2, horiz = TRUE, cex.names = 0.7,
        col = colorRampPalette(colors = c("darkred","white","darkblue"))(19))
```

Variable Importance:
lcavol  > lweight > svi > lcp > pgg45 > age > lbph > gleason

## E. Boosting 

```{r}
set.seed(1)

gbm.grid = expand.grid(n.trees = seq(1,5001, by = 500),
                       interaction.depth = 1:10,
                       shrinkage = c(0.001, 0.003, 0.005),
                       n.minobsinnode = 1)

gbm_fit = train(lpsa~.,
                data = Prostate,
                method = "gbm",
                tuneGrid = gbm.grid,
                trControl = trainControl(method = "cv"),
                verbose = FALSE)

ggplot(gbm_fit, highlight = TRUE)

summary(gbm_fit$finalModel, las = 2, cBars = 19, cex.names = 0.6)
```

Variable Importance:
lcavol > lweight > svi > lcp > pgg45 > age > lbph > gleason

## F. Comparisons

```{r}
resamp_2 = resamples(list(tree_fit = tree_fit_1, 
                          tree_fit_1SE = tree_fit_2,
                          bagging = bag_fit,
                          randomforest = rf_fit,
                          boosting = gbm_fit))

summary(resamp_2)
bwplot(resamp_2, metric = "RMSE")
ggplot(resamp_2, metric = "RMSE")
```

Boosting, random forests and bagging have similar RMSE mean and median. However, random forests has the largest mean R-squared value. Therefore, random forests is chosen to predict PSA level.

# Problem 2

## A. Decision Tree

```{r}
set.seed(1)
trainindex = sample(seq_len(nrow(OJ)), size = 800)
train = OJ[trainindex, ]
test = OJ[-trainindex, ]

ctrl2 = trainControl(method = "repeatedcv", summaryFunction = twoClassSummary, classProbs = TRUE)

rpart_fit_c = train(Purchase~.,
                    data = train,
                    method = "rpart",
                    tuneGrid = data.frame(cp = exp(seq(-15,0, by = 2))),
                    trControl = ctrl2,
                    metric = "ROC")

plot(rpart_fit_c, xTrans = function(x)log(x), xlab = "log(cp)")
ggplot(rpart_fit_c, highlight = TRUE)

rpart_fit_c$bestTune
rpart_fit_c$finalModel$cptable

rpart.plot(rpart_fit_c$finalModel) # final tree

tree.pred = predict(rpart_fit_c, newdata = test, type = "raw")
1 - sum(tree.pred == test$Purchase) / length(test$Purchase)
```

The final tree has a size of 20. The test classification error rate is 17.40741%.

## B. Random forests

```{r}
set.seed(1)

rf.grid2 = expand.grid(mtry = 2:7,
                       splitrule = "gini",
                       min.node.size = seq(20,120, by = 10))

rf_fit_c = train(Purchase~.,
                 data = train,
                 method = "ranger",
                 tuneGrid = rf.grid2,
                 metric = "ROC",
                 trControl = ctrl2,
                 importance = "impurity")

ggplot(rf_fit_c, highlight = TRUE)

barplot(sort(ranger::importance(rf_fit_c$finalModel),
             decreasing = FALSE),
        las = 2, horiz = TRUE, cex.names = 0.7,
        col = colorRampPalette(colors = c("darkred","white","darkblue"))(19))

rf.pred = predict(rf_fit_c, newdata = test, type = "raw")
1 - sum(rf.pred == test$Purchase) / length(test$Purchase)
```

Variables Importance:
LoyalCH > StoreID > PriceDiff > WeekofPurchase > STORE > SalePriceMM > ListPriceDiff > SalePriceCH > PctDiscMM > Store7Yes > PriceMM > PctDiscCH > DiscCH > DiscMM > SpecialCH > PriceCH > SpecialMM

The test classification error rate is 15.55556%.


## C. Boosting

```{r}
set.seed(1)

gbm.grid2 = expand.grid(n.trees = seq(100, 600, by = 10),
                        interaction.depth = 2:6,
                        shrinkage = c(0.001, 0.003, 0.005),
                        n.minobsinnode = 1)

gbm_fit_c = train(Purchase~.,
                  data = train,
                  method = "gbm",
                  trControl = ctrl2,
                  distribution = "bernoulli",
                  metric = "ROC",
                  tuneGrid = gbm.grid2,
                  verbose = FALSE)

ggplot(gbm_fit_c, highlight = TRUE)

summary(gbm_fit_c$finalModel, las = 2, cBars = 19, cex.names = 0.6)

gbm.pred = predict(gbm_fit_c, newdata = test, type = "raw")
1 - sum(gbm.pred == test$Purchase) / length(test$Purchase)
```


Variable Importance:
LoyalCH > PriceDiff > StoreID > SalePriceMM > ListPriceDiff > STORE > WeekofPurchase > Store7Yes > SpecialCH > DiscCH > SalePriceCH > SpecialMM > PriceMM > PctDiscCH > DiscMM > PriceCH > PctDiscMM

The test classification error rate is 14.81481%.

